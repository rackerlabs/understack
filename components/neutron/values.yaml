---
release_group: null

# typically overridden by environmental
# values, but should include all endpoints
# required by this chart
endpoints:
  oslo_messaging:
    statefulset:
      replicas: 3
      name: rabbitmq-server
    hosts:
      default: rabbitmq-nodes
  network:
    port:
      api:
        public: 443
    scheme:
      public: https
    host_fqdn_override:
      public:
        tls:
          secretName: neutron-tls-public
          issuerRef:
            name: understack-cluster-issuer
            kind: ClusterIssuer


network:
  # we're using ironic and actual switches so baremetal
  # we're using OVN for our router solution
  backend:
    - baremetal
    - ovn

  # configure OpenStack Helm to use Undercloud's ingress
  # instead of expecting the ingress controller provided
  # by OpenStack Helm
  use_external_ingress_controller: true
  server:
    ingress:
      annotations:
        nginx.ingress.kubernetes.io/rewrite-target: /
        # set our default issuer
        cert-manager.io/cluster-issuer: understack-cluster-issuer

conf:
  plugins:
    ml2_conf:
      ml2:
        # at this time due to physical switches not doing OpenFlow and enabling
        # port security rules being different per model (or supported at all)
        # disable it by default. this is necessary because openstack-helm enables
        # it by default
        extension_drivers: ''
        # set the default ml2 backend to our plugin, neutron_understack
        # we'll need to use the ovn ML2 plugin to hook the routers to our network
        mechanism_drivers: "understack,ovn"
        tenant_network_types: "vxlan"
        type_drivers: "vlan,vxlan"
      ml2_type_vxlan:
        # OSH sets a default range here but we want to use network_segment_range plugin
        # to configure this instead
        vni_ranges: ""
  neutron:
    DEFAULT:
      # We enable the following plugins:
      # - 'ovn-router' enables OVN to be our L3 router.
      # - 'trunk' allows for us to create and configure trunk ports to allow
      # multiple networks to be trunked to a node and let the node apply the
      # VLAN
      # - 'network_segment_range' allows us to set the allowed VNIs or VLANs for
      # a given network and let's OpenStack select one from the available pool.
      # We are also able to see which ones are used from the OpenStack API.
      # - 'segments' enables the /segments API for CRUD operations on network
      # segments.
      service_plugins: "ovn-router,trunk,network_segment_range,segments"
      # we don't want HA L3 routers. It's a Python value so we need to quote it in YAML.
      l3_ha: "False"
      # we aren't using availability zones so having calls attempt to add things to
      # availability zones won't work.
      default_availability_zones: ""
      # add 50 to the max MTU we want of 9000 to handle Neutron's -50 for VXLAN type
      global_physnet_mtu: 9050
    service_providers:
      service_provider:
        type: multistring
        values:
          - "L3_ROUTER_NAT:cisco-asa:neutron_understack.l3_router.cisco_asa.CiscoAsa"
          - "L3_ROUTER_NAT:palo-alto:neutron_understack.l3_router.palo_alto.PaloAlto"
          - "L3_ROUTER_NAT:vrf:neutron_understack.l3_router.vrf.Vrf"
          - "L3_ROUTER_NAT:svi:neutron_understack.l3_router.svi.Svi"
    ovn:
      # the ovn-metadata-agent utilizes 'localport' on each hypervisor in OVS to work, since
      # we don't have an OVS that the baremetal nodes are plugged into we can't have this
      # working at this time so we need to disable it
      ovn_metadata_enabled: false
      # by default let's have OVN's DB be updated with what is in neutron on startup
      neutron_sync_mode: repair
    quotas:
      # https://github.com/openstack/neutron/blob/master/neutron/conf/quota.py#L101-L105
      quota_rbac_policy: 100

# disable the neutron-ironic-agent from loading a non-existent config
pod:
  use_fqdn:
    neutron_agent: false
  lifecycle:
    disruption_budget:
      server:
        # this should be set to no more than (pod.replicas.server - 1)
        # usually set on per-deployment basis.
        min_available: 0
  mounts:
    neutron_server:
      neutron_server:
        volumeMounts:
          # oslo.config autoloads certain paths in alphabetical order
          # which gives us the opportunity to inject secrets and extra
          # configs here. likely the best paths are:
          # /etc/${project}/${prog}.conf.d/*.conf
          # /etc/${project}/${project}.conf.d/*.conf
          # the first would be best for per service separation but since each
          # service is in its own pod they won't overlap. further more there
          # is an issue with that see https://bugs.launchpad.net/oslo.config/+bug/2098514
          # so we'll use the bottom one
          - mountPath: /etc/neutron/neutron.conf.d/ml2_understack.conf
            name: neutron-nautobot
            subPath: ml2_understack.conf
            readOnly: true
          - mountPath: /etc/undersync/
            name: undersync-token
            readOnly: true
        volumes:
          - name: neutron-nautobot
            secret:
              secretName: neutron-nautobot
          - name: undersync-token
            secret:
              secretName: undersync-token
# (nicholas.kuechler) updating the jobs list to remove the 'neutron-rabbit-init' job.
dependencies:
  dynamic:
    common:
      local_image_registry:
        jobs: null
  static:
    db_sync:
      jobs:
    dhcp:
      jobs:
    l3:
      jobs:
    lb_agent:
      jobs:
    metadata:
      jobs:
    ovs_agent:
      jobs:
    server:
      jobs:
        - neutron-db-sync
        - neutron-ks-user
        - neutron-ks-endpoints
    rpc_server:
      jobs:
        - neutron-db-sync
    ironic_agent:
      jobs:
        - neutron-db-sync
        - neutron-ks-user
        - neutron-ks-endpoints

manifests:
  job_db_init: false
  job_rabbit_init: false
  pod_rally_test: false
  secret_db: false
  secret_keystone: true
  # OVN has its own pieces and does not need the following:
  # - neutron-dhcp-agent (OVN's does not work with baremetal/ironic currently)
  # - neutron-l3-agent
  # - neutron-metadata-agent
  # - neutron-ovs-agent
  # - neutron-sriov-agent
  # - neutron-rpc-agent (this sends requests to the various agents)
  # https://docs.openstack.org/ironic/latest/admin/ovn-networking.html
  daemonset_dhcp_agent: false
  daemonset_l3_agent: false
  daemonset_lb_agent: false
  daemonset_metadata_agent: false
  daemonset_ovs_agent: false
  deployment_rpc_server: false
  daemonset_sriov_agent: false
  daemonset_l2gw_agent: false
  daemonset_bagpipe_bgp: false
  daemonset_bgp_dragent: false
  daemonset_netns_cleanup_cron: false
  deployment_ironic_agent: true
  service_ingress_server: false

annotations:
  # we need to modify the annotations on OpenStack Helm
  # jobs because they use helm.sh/hooks: post-install,post-upgrade
  # which means they will get applied in the post phase which
  # is after the API deployment. With standard helm this works
  # out because it just orders how things are applied but with
  # ArgoCD it will wait until the sync phase is successful.
  # Unfortunately the API deployments need several jobs to occur
  # before it will go successful like creating the keystone user,
  # service, endpoints and syncing the DB. These jobs also have
  # a helm.sh/hook-weight to order them which is good but by moving
  # them to the sync phase the weight is now wrong with resources
  # they depend on like secrets and configmaps so we need to
  # override them to 0 because there is no way in OpenStack Helm
  # to set annotations on deployments and daemonssets nicely.
  # Other jobs might need to be moved as well. We do this by
  # moving them to the sync phase. Additionally since the jobs
  # are using fixed names and not generated names for each run
  # ArgoCD attempts to edit them but they have immutable fields
  # so we must force the replacement instead of attempting to diff them.
  # Lastly the hook-delete-policy controls the finalizer which
  # prevents the deletion of the job. In this case we're saying
  # the old job needs to be removed before applying the new one
  # which gets around the immutable case above.
  job:
    neutron_db_sync:
      argocd.argoproj.io/hook: Sync
      argocd.argoproj.io/hook-delete-policy: BeforeHookCreation
      argocd.argoproj.io/sync-options: Force=true
      argocd.argoproj.io/sync-wave: "0"
    neutron_ks_service:
      argocd.argoproj.io/hook: Sync
      argocd.argoproj.io/hook-delete-policy: BeforeHookCreation
      argocd.argoproj.io/sync-options: Force=true
      argocd.argoproj.io/sync-wave: "0"
    neutron_ks_user:
      argocd.argoproj.io/hook: Sync
      argocd.argoproj.io/hook-delete-policy: BeforeHookCreation
      argocd.argoproj.io/sync-options: Force=true
      argocd.argoproj.io/sync-wave: "0"
    neutron_ks_endpoints:
      argocd.argoproj.io/hook: Sync
      argocd.argoproj.io/hook-delete-policy: BeforeHookCreation
      argocd.argoproj.io/sync-options: Force=true
      argocd.argoproj.io/sync-wave: "0"
