---
release_group: null

# typically overridden by environmental
# values, but should include all endpoints
# required by this chart
endpoints:
  oslo_messaging:
    statefulset:
      replicas: 3
      name: rabbitmq-server
    hosts:
      default: rabbitmq-nodes
  network:
    port:
      api:
        public: 443
    scheme:
      public: https
    host_fqdn_override:
      public:
        tls:
          secretName: neutron-tls-public
          issuerRef:
            name: understack-cluster-issuer
            kind: ClusterIssuer


network:
  # we're using ironic and actual switches so baremetal
  # we're using OVN for our router solution
  backend:
    - baremetal
    - ovn

  # configure OpenStack Helm to use Undercloud's ingress
  # instead of expecting the ingress controller provided
  # by OpenStack Helm
  use_external_ingress_controller: true
  server:
    ingress:
      annotations:
        nginx.ingress.kubernetes.io/rewrite-target: /
        # set our default issuer
        cert-manager.io/cluster-issuer: understack-cluster-issuer

conf:
  plugins:
    ml2_conf:
      ml2:
        extension_drivers: 'port_security'
        # set the default ml2 backend to our plugin, neutron_understack
        # we'll need to use the ovn ML2 plugin to hook the routers to our network
        mechanism_drivers: "understack,ovn"
        tenant_network_types: "vxlan"
        type_drivers: "vlan,vxlan"
      ml2_type_vxlan:
        # OSH sets a default range here but we want to use network_segment_range plugin
        # to configure this instead
        vni_ranges: ""
  neutron:
    DEFAULT:
      # https://docs.openstack.org/neutron/latest/admin/config-wsgi.html
      # the api_workers set the number of uWSGI processes as well
      api_workers: 4
      rpc_workers: 2
      # We enable the following plugins:
      # - 'ovn-router' enables OVN to be our L3 router.
      # - 'trunk' allows for us to create and configure trunk ports to allow
      # multiple networks to be trunked to a node and let the node apply the
      # VLAN
      # - 'network_segment_range' allows us to set the allowed VNIs or VLANs for
      # a given network and let's OpenStack select one from the available pool.
      # We are also able to see which ones are used from the OpenStack API.
      # - 'segments' enables the /segments API for CRUD operations on network
      # segments.
      service_plugins: "ovn-router,trunk,network_segment_range,segments"
      # we don't want HA L3 routers. It's a Python value so we need to quote it in YAML.
      l3_ha: "False"
      # we aren't using availability zones so having calls attempt to add things to
      # availability zones won't work.
      default_availability_zones: ""
      # add 50 to the max MTU we want of 9000 to handle Neutron's -50 for VXLAN type
      global_physnet_mtu: 9050
    service_providers:
      service_provider:
        type: multistring
        values:
          - "L3_ROUTER_NAT:cisco-asa:neutron_understack.l3_router.cisco_asa.CiscoAsa"
          - "L3_ROUTER_NAT:palo-alto:neutron_understack.l3_router.palo_alto.PaloAlto"
          - "L3_ROUTER_NAT:vrf:neutron_understack.l3_router.vrf.Vrf"
          - "L3_ROUTER_NAT:svi:neutron_understack.l3_router.svi.Svi"
    ovn:
      # the ovn-metadata-agent utilizes 'localport' on each hypervisor in OVS to work, since
      # we don't have an OVS that the baremetal nodes are plugged into we can't have this
      # working at this time so we need to disable it
      ovn_metadata_enabled: false
      # by default let's have OVN's DB be updated with what is in neutron on startup
      neutron_sync_mode: repair
    quotas:
      # https://github.com/openstack/neutron/blob/master/neutron/conf/quota.py#L101-L105
      quota_rbac_policy: 100
  neutron_api_uwsgi:
    uwsgi:
      start-time: "%t"

# disable the neutron-ironic-agent from loading a non-existent config
pod:
  mounts:
    neutron_db_sync:
      neutron_db_sync:
        volumeMounts:
          # this is upstream but since we're making a list it gets replaced
          - name: db-sync-conf
            mountPath: /etc/neutron/plugins/ml2/ml2_conf.ini
            subPath: ml2_conf.ini
            readOnly: true
          - name: neutron-etc-snippets
            mountPath: /etc/neutron/neutron.conf.d/
            readOnly: true
        volumes:
          - name: neutron-etc-snippets
            projected:
              sources:
                - secret:
                    name: neutron-ks-etc
    neutron_ironic_agent:
      neutron_ironic_agent:
        volumeMounts:
          - name: neutron-etc-snippets
            mountPath: /etc/neutron/neutron.conf.d/
            readOnly: true
        volumes:
          - name: neutron-etc-snippets
            projected:
              sources:
                - secret:
                    name: neutron-ks-etc
    neutron_server:
      neutron_server:
        volumeMounts:
          - mountPath: /etc/undersync/
            name: undersync-token
            readOnly: true
          - name: neutron-etc-snippets
            mountPath: /etc/neutron/neutron.conf.d/
            readOnly: true
        volumes:
          - name: undersync-token
            secret:
              secretName: undersync-token
          - name: neutron-etc-snippets
            projected:
              sources:
                - secret:
                    name: neutron-ks-etc

  use_fqdn:
    neutron_agent: false
  lifecycle:
    disruption_budget:
      server:
        # this should be set to no more than (pod.replicas.server - 1)
        # usually set on per-deployment basis.
        min_available: 0
  resources:
    enabled: true
    server:
      requests:
        memory: "2048Mi"
        cpu: "200m"
      limits:
        memory: "4096Mi"

# (nicholas.kuechler) updating the jobs list to remove the 'neutron-rabbit-init' job.
dependencies:
  dynamic:
    common:
      local_image_registry:
        jobs: null
  static:
    db_sync:
      jobs:
    dhcp:
      jobs:
    l3:
      jobs:
    lb_agent:
      jobs:
    metadata:
      jobs:
    ovs_agent:
      jobs:
    server:
      jobs:
        - neutron-db-sync
        - neutron-ks-user
        - neutron-ks-endpoints
    rpc_server:
      jobs:
        - neutron-db-sync
    ironic_agent:
      jobs:
        - neutron-db-sync
        - neutron-ks-user
        - neutron-ks-endpoints

manifests:
  job_db_init: false
  job_rabbit_init: false
  pod_rally_test: false
  secret_db: false
  # OVN has its own pieces and does not need the following:
  # - neutron-dhcp-agent (OVN's does not work with baremetal/ironic currently)
  # - neutron-l3-agent
  # - neutron-metadata-agent
  # - neutron-ovs-agent
  # - neutron-sriov-agent
  # - neutron-rpc-agent (this sends requests to the various agents)
  # https://docs.openstack.org/ironic/latest/admin/ovn-networking.html
  daemonset_dhcp_agent: false
  daemonset_l3_agent: false
  daemonset_lb_agent: false
  daemonset_metadata_agent: false
  daemonset_ovs_agent: false
  deployment_rpc_server: false
  daemonset_sriov_agent: false
  daemonset_l2gw_agent: false
  daemonset_bagpipe_bgp: false
  daemonset_bgp_dragent: false
  daemonset_netns_cleanup_cron: false
  deployment_ironic_agent: true
  service_ingress_server: false
  # We set the `secret_keystone` and `secret_ks_etc` to false in order to disable
  # Kubernetes section generation in OpenStack Helm, because we want those
  # to be generated indirectly via ESO as configured in keystoneServiceUsers.enabled
  # that is later consumed via components/openstack helm chart
  secret_keystone: false
  secret_ks_etc: false
  # less than ideal but currently necessary due to the way the chart
  # hardcodes the config-file flag which then causes oslo.config to prefer
  # that data over the directory which we want to override
  configmap_bin: false

annotations:
  # we need to modify the annotations on OpenStack Helm
  # jobs because they use helm.sh/hooks: post-install,post-upgrade
  # which means they will get applied in the post phase which
  # is after the API deployment. With standard helm this works
  # out because it just orders how things are applied but with
  # ArgoCD it will wait until the sync phase is successful.
  # Unfortunately the API deployments need several jobs to occur
  # before it will go successful like creating the keystone user,
  # service, endpoints and syncing the DB. These jobs also have
  # a helm.sh/hook-weight to order them which is good but by moving
  # them to the sync phase the weight is now wrong with resources
  # they depend on like secrets and configmaps so we need to
  # override them to 0 because there is no way in OpenStack Helm
  # to set annotations on deployments and daemonssets nicely.
  # Other jobs might need to be moved as well. We do this by
  # moving them to the sync phase. Additionally since the jobs
  # are using fixed names and not generated names for each run
  # ArgoCD attempts to edit them but they have immutable fields
  # so we must force the replacement instead of attempting to diff them.
  # Lastly the hook-delete-policy controls the finalizer which
  # prevents the deletion of the job. In this case we're saying
  # the old job needs to be removed before applying the new one
  # which gets around the immutable case above.
  job:
    neutron_db_sync:
      argocd.argoproj.io/hook: Sync
      argocd.argoproj.io/hook-delete-policy: BeforeHookCreation
      argocd.argoproj.io/sync-options: Force=true
      argocd.argoproj.io/sync-wave: "0"
    neutron_ks_service:
      argocd.argoproj.io/hook: Sync
      argocd.argoproj.io/hook-delete-policy: BeforeHookCreation
      argocd.argoproj.io/sync-options: Force=true
      argocd.argoproj.io/sync-wave: "0"
    neutron_ks_user:
      argocd.argoproj.io/hook: Sync
      argocd.argoproj.io/hook-delete-policy: BeforeHookCreation
      argocd.argoproj.io/sync-options: Force=true
      argocd.argoproj.io/sync-wave: "0"
    neutron_ks_endpoints:
      argocd.argoproj.io/hook: Sync
      argocd.argoproj.io/hook-delete-policy: BeforeHookCreation
      argocd.argoproj.io/sync-options: Force=true
      argocd.argoproj.io/sync-wave: "0"
