---
release_group: null

# temporarily set this to the same as the control plane
labels:
  agent:
    compute_ironic:
      node_selector_key: openstack-control-plane
      node_selector_value: enabled

# typically overridden by environmental
# values, but should include all endpoints
# required by this chart
endpoints:
  oslo_messaging:
    statefulset:
      replicas: 3
      name: rabbitmq-server
    hosts:
      default: rabbitmq-nodes
  compute:
    port:
      api:
        public: 443
    scheme:
      public: https
    host_fqdn_override:
      public:
        tls:
          secretName: nova-tls-public
          issuerRef:
            name: understack-cluster-issuer
            kind: ClusterIssuer

network:
  # we're using ironic and actual switches
  backend:
    - baremetal

  # configure OpenStack Helm to use Undercloud's ingress
  # instead of expecting the ingress controller provided
  # by OpenStack Helm
  use_external_ingress_controller: true
  osapi:
    ingress:
      annotations:
        nginx.ingress.kubernetes.io/rewrite-target: /
        # set our default issuer
        cert-manager.io/cluster-issuer: understack-cluster-issuer

conf:
  ceph:
    # ceph is providing block storage to VM creation and is connected via libvirt
    # we aren't using this so we don't want to enable this part of the chart
    enabled: false
  DEFAULT:
    # We are not wiring up the network to the nova metadata API so we must use
    # config_drive to pass data. To avoid users having to remember this, just
    # force it on always.
    force_config_drive: true
  nova_ironic:
    ironic:
      # this is where we populate our hardware
      project_domain_name: infra
      project_name: baremetal
  nova:
    quota:
      # adjust default quotas to make it possible to use baremetal
      cores: 512
      ram: 512000

    # (TODO) This is to help with an upstream Nova bug:
    # https://review.opendev.org/c/openstack/nova/+/883411
    #
    # This can be removed from when the upstream issue has been resolved.
    # Cleaning can take around 5-10 minutes, so we need the value of
    # (api_max_retries * api_retry_interval) > time to clean
    ironic:
      api_max_retries: 90  # number of times to retry. default is 60.
      api_retry_interval: 10  # number of sesconds between retries. default is 2.

console:
  # we are working with baremetal nodes and not QEMU so we don't need novnc or spice
  # connected to QEMU
  console_kind: none

# (nicholas.kuechler) Using custom dependencies in order to
# prevent the nova-db-init and nova-rabbit-init jobs from running
dependencies:
  dynamic:
    common:
      local_image_registry:
        jobs: null
  static:
    api:
      jobs:
        - nova-db-sync
        - nova-ks-user
        - nova-ks-endpoints
    api_metadata:
      jobs:
        - nova-db-sync
        - nova-ks-user
        - nova-ks-endpoints
    cell_setup:
      jobs:
        - nova-db-sync
      # remove default dependency to run on the same node as a compute service
      pod: []
    service_cleaner:
      jobs:
        - nova-db-sync
    compute:
      pod: []
      jobs:
        - nova-db-sync
    compute_ironic:
      jobs:
        - nova-db-sync
      # this chunk is here just to disable waiting on glance/image service
      services:
        - endpoint: internal
          service: oslo_messaging
        - endpoint: internal
          service: compute
        - endpoint: internal
          service: network
        - endpoint: internal
          service: baremetal
    conductor:
      jobs:
        - nova-db-sync
    archive_deleted_rows:
      jobs:
        - nova-db-sync
    db_sync:
      jobs:
    scheduler:
      jobs:
        - nova-db-sync

pod:
  lifecycle:
    disruption_budget:
      osapi:
        # this should be set to no more than (pod.replicas.osapi - 1)
        # usually set on per-deployment basis.
        min_available: 0
  mounts:
    # oslo.config autoloads certain paths in alphabetical order
    # which gives us the opportunity to inject secrets and extra
    # configs here. likely the best paths are:
    # /etc/${project}/${prog}.conf.d/*.conf
    # /etc/${project}/${project}.conf.d/*.conf
    # the first would be best for per service separation but since each
    # service is in its own pod they won't overlap. further more there
    # is an issue with that see https://bugs.launchpad.net/oslo.config/+bug/2098514
    # so we'll use the bottom one
    nova_compute_ironic:
      nova_compute_ironic:
        volumeMounts:
          - mountPath: /etc/nova/nova.conf.d/db_conn.conf
            name: nova-db-conn
            subPath: db_conf.conf
            readOnly: true
          - mountPath: /etc/nova/nova.conf.d/api_db_conn.conf
            name: nova-api-db-conn
            subPath: db_conf.conf
            readOnly: true
          - mountPath: /etc/nova/nova.conf.d/cell0_db_conn.conf
            name: nova-cell0-db-conn
            subPath: db_conf.conf
            readOnly: true
        volumes:
          - name: nova-db-conn
            secret:
              secretName: nova-db-conn
          - name: nova-api-db-conn
            secret:
              secretName: nova-api-db-conn
          - name: nova-cell0-db-conn
            secret:
              secretName: nova-cell0-db-conn
    nova_api_osapi:
      nova_api_osapi:
        volumeMounts:
          - mountPath: /etc/nova/nova.conf.d/db_conn.conf
            name: nova-db-conn
            subPath: db_conf.conf
            readOnly: true
          - mountPath: /etc/nova/nova.conf.d/api_db_conn.conf
            name: nova-api-db-conn
            subPath: db_conf.conf
            readOnly: true
          - mountPath: /etc/nova/nova.conf.d/cell0_db_conn.conf
            name: nova-cell0-db-conn
            subPath: db_conf.conf
            readOnly: true
        volumes:
          - name: nova-db-conn
            secret:
              secretName: nova-db-conn
          - name: nova-api-db-conn
            secret:
              secretName: nova-api-db-conn
          - name: nova-cell0-db-conn
            secret:
              secretName: nova-cell0-db-conn
    nova_conductor:
      nova_conductor:
        volumeMounts:
          - mountPath: /etc/nova/nova.conf.d/db_conn.conf
            name: nova-db-conn
            subPath: db_conf.conf
            readOnly: true
          - mountPath: /etc/nova/nova.conf.d/api_db_conn.conf
            name: nova-api-db-conn
            subPath: db_conf.conf
            readOnly: true
          - mountPath: /etc/nova/nova.conf.d/cell0_db_conn.conf
            name: nova-cell0-db-conn
            subPath: db_conf.conf
            readOnly: true
        volumes:
          - name: nova-db-conn
            secret:
              secretName: nova-db-conn
          - name: nova-api-db-conn
            secret:
              secretName: nova-api-db-conn
          - name: nova-cell0-db-conn
            secret:
              secretName: nova-cell0-db-conn
    nova_scheduler:
      nova_scheduler:
        volumeMounts:
          - mountPath: /etc/nova/nova.conf.d/db_conn.conf
            name: nova-db-conn
            subPath: db_conf.conf
            readOnly: true
          - mountPath: /etc/nova/nova.conf.d/api_db_conn.conf
            name: nova-api-db-conn
            subPath: db_conf.conf
            readOnly: true
          - mountPath: /etc/nova/nova.conf.d/cell0_db_conn.conf
            name: nova-cell0-db-conn
            subPath: db_conf.conf
            readOnly: true
        volumes:
          - name: nova-db-conn
            secret:
              secretName: nova-db-conn
          - name: nova-api-db-conn
            secret:
              secretName: nova-api-db-conn
          - name: nova-cell0-db-conn
            secret:
              secretName: nova-cell0-db-conn
    nova_db_sync:
      nova_db_sync:
        volumeMounts:
          - mountPath: /etc/nova/nova.conf.d/db_conn.conf
            name: nova-db-conn
            subPath: db_conf.conf
            readOnly: true
          - mountPath: /etc/nova/nova.conf.d/api_db_conn.conf
            name: nova-api-db-conn
            subPath: db_conf.conf
            readOnly: true
          - mountPath: /etc/nova/nova.conf.d/cell0_db_conn.conf
            name: nova-cell0-db-conn
            subPath: db_conf.conf
            readOnly: true
        volumes:
          - name: nova-db-conn
            secret:
              secretName: nova-db-conn
          - name: nova-api-db-conn
            secret:
              secretName: nova-api-db-conn
          - name: nova-cell0-db-conn
            secret:
              secretName: nova-cell0-db-conn

manifests:
  job_db_init: false
  job_rabbit_init: false
  job_storage_init: false
  pod_rally_test: false
  secret_db_api: true
  secret_db_cell0: true
  secret_db: true
  secret_keystone: true
  service_ingress_metadata: false
  service_ingress_osapi: false
  daemonset_compute: false
  statefulset_compute_ironic: true

# we don't want to enable OpenStack Helm's
# helm.sh/hooks because they set them as
# post-install,post-upgrade which in ArgoCD
# maps to PostSync. However the deployments
# and statefulsets in OpenStack Helm
# depend on the jobs to complete to become
# healthy. Which they cannot because they are in
# the post step and not in the main step.
# Turning this on results in the keys jobs
# editing the annotation which deletes the item
# and wipes our keys.
helm3_hook: false

annotations:
  job:
    nova_db_sync:
      argocd.argoproj.io/hook: Sync
      argocd.argoproj.io/hook-delete-policy: BeforeHookCreation
      argocd.argoproj.io/sync-options: Replace=true
    nova_ks_service:
      argocd.argoproj.io/hook: Sync
      argocd.argoproj.io/hook-delete-policy: BeforeHookCreation
      argocd.argoproj.io/sync-options: Replace=true
    nova_ks_user:
      argocd.argoproj.io/hook: Sync
      argocd.argoproj.io/hook-delete-policy: BeforeHookCreation
      argocd.argoproj.io/sync-options: Replace=true
    nova_ks_endpoints:
      argocd.argoproj.io/hook: Sync
      argocd.argoproj.io/hook-delete-policy: BeforeHookCreation
      argocd.argoproj.io/sync-options: Replace=true
    nova_cell_setup:
      argocd.argoproj.io/hook: PostSync
      argocd.argoproj.io/hook-delete-policy: BeforeHookCreation
      argocd.argoproj.io/sync-options: Force=true,Replace=true
    nova_bootstrap:
      argocd.argoproj.io/hook: Sync
      argocd.argoproj.io/hook-delete-policy: BeforeHookCreation
      argocd.argoproj.io/sync-options: Replace=true
