---
release_group: null

# temporarily set this to the same as the control plane
labels:
  agent:
    compute_ironic:
      node_selector_key: openstack-control-plane
      node_selector_value: enabled

# typically overridden by environmental
# values, but should include all endpoints
# required by this chart
endpoints:
  oslo_messaging:
    statefulset:
      replicas: 3
      name: rabbitmq-server
    hosts:
      default: rabbitmq-nodes
  compute:
    port:
      api:
        public: 443
    scheme:
      public: https
    host_fqdn_override:
      public:
        tls:
          secretName: nova-tls-public
          issuerRef:
            name: understack-cluster-issuer
            kind: ClusterIssuer

network:
  # we're using ironic and actual switches
  backend:
    - baremetal

  # configure OpenStack Helm to use Undercloud's ingress
  # instead of expecting the ingress controller provided
  # by OpenStack Helm
  use_external_ingress_controller: true
  osapi:
    ingress:
      annotations:
        nginx.ingress.kubernetes.io/rewrite-target: /
        # set our default issuer
        cert-manager.io/cluster-issuer: understack-cluster-issuer

conf:
  ceph:
    # ceph is providing block storage to VM creation and is connected via libvirt
    # we aren't using this so we don't want to enable this part of the chart
    enabled: false
  DEFAULT:
    # We are not wiring up the network to the nova metadata API so we must use
    # config_drive to pass data. To avoid users having to remember this, just
    # force it on always.
    force_config_drive: true
  nova_ironic:
    ironic:
      # this is where we populate our hardware
      project_domain_name: infra
      project_name: baremetal
  nova:
    quota:
      # adjust default quotas to make it possible to use baremetal
      cores: 512
      ram: "1024000"
    api:
      dhcp_domain: ""

    # (TODO) This is to help with an upstream Nova bug:
    # https://review.opendev.org/c/openstack/nova/+/883411
    #
    # This can be removed from when the upstream issue has been resolved.
    # Cleaning can take around 5-10 minutes, so we need the value of
    # (api_max_retries * api_retry_interval) > time to clean
    ironic:
      api_max_retries: 90  # number of times to retry. default is 60.
      api_retry_interval: 10  # number of sesconds between retries. default is 2.

    # https://docs.openstack.org/nova/2025.1/admin/scheduling.html#the-filter-scheduler
    filter_scheduler:
      available_filters: nova.scheduler.filters.all_filters
      enabled_filters:
        - ComputeFilter
        - ComputeCapabilitiesFilter
        - ImagePropertiesFilter
        - ServerGroupAntiAffinityFilter
        - ServerGroupAffinityFilter
        - JsonFilter

console:
  # we are working with baremetal nodes and not QEMU so we don't need novnc or spice
  # connected to QEMU
  console_kind: none

bootstrap:
  structured:
    flavors:
      # this script adds hardcoded flavors which we never use so disable it
      enabled: false

# (nicholas.kuechler) Using custom dependencies in order to
# prevent the nova-db-init and nova-rabbit-init jobs from running
dependencies:
  dynamic:
    common:
      local_image_registry:
        jobs: null
  static:
    api:
      jobs:
        - nova-db-sync
        - nova-ks-user
        - nova-ks-endpoints
    api_metadata:
      jobs:
        - nova-db-sync
        - nova-ks-user
        - nova-ks-endpoints
    cell_setup:
      jobs:
        - nova-db-sync
      # remove default dependency to run on the same node as a compute service
      pod: []
    service_cleaner:
      jobs:
        - nova-db-sync
    compute:
      pod: []
      jobs:
        - nova-db-sync
    compute_ironic:
      jobs:
        - nova-db-sync
      # this chunk is here just to disable waiting on glance/image service
      services:
        - endpoint: internal
          service: oslo_messaging
        - endpoint: internal
          service: compute
        - endpoint: internal
          service: network
        - endpoint: internal
          service: baremetal
    conductor:
      jobs:
        - nova-db-sync
    archive_deleted_rows:
      jobs:
        - nova-db-sync
    db_sync:
      jobs:
    scheduler:
      jobs:
        - nova-db-sync

pod:
  etcSources:
    nova_db_sync:
      - secret:
          name: nova-ks-etc
    nova_api_osapi:
      - secret:
          name: nova-ks-etc
    nova_api_metadata:
      - secret:
          name: nova-ks-etc
    nova_compute_ironic:
      - secret:
          name: nova-ks-etc
    nova_scheduler:
      - secret:
          name: nova-ks-etc
    nova_conductor:
      - secret:
          name: nova-ks-etc
      - secret:
          name: nova-nautobot
    nova_archive_deleted_rows:
      - secret:
          name: nova-ks-etc
    nova_service_cleaner:
      - secret:
          name: nova-ks-etc
    nova_cell_setup:
      - secret:
          name: nova-ks-etc
  lifecycle:
    disruption_budget:
      osapi:
        # this should be set to no more than (pod.replicas.osapi - 1)
        # usually set on per-deployment basis.
        min_available: 0
  resources:
    enabled: true

manifests:
  job_db_init: false
  job_rabbit_init: false
  job_storage_init: false
  pod_rally_test: false
  secret_db_api: true
  secret_db_cell0: true
  secret_db: true
  service_ingress_metadata: false
  service_ingress_osapi: false
  daemonset_compute: false
  statefulset_compute_ironic: true
  # We set the `secret_keystone` and `secret_ks_etc` to false in order to disable
  # Kubernetes section generation in OpenStack Helm, because we want those
  # to be generated indirectly via ESO as configured in keystoneServiceUsers.enabled
  # that is later consumed via components/openstack helm chart
  secret_keystone: false
  secret_ks_etc: false

annotations:
  # we need to modify the annotations on OpenStack Helm
  # jobs because they use helm.sh/hooks: post-install,post-upgrade
  # which means they will get applied in the post phase which
  # is after the API deployment. With standard helm this works
  # out because it just orders how things are applied but with
  # ArgoCD it will wait until the sync phase is successful.
  # Unfortunately the API deployments need several jobs to occur
  # before it will go successful like creating the keystone user,
  # service, endpoints and syncing the DB. These jobs also have
  # a helm.sh/hook-weight to order them which is good but by moving
  # them to the sync phase the weight is now wrong with resources
  # they depend on like secrets and configmaps so we need to
  # override them to 0 because there is no way in OpenStack Helm
  # to set annotations on deployments and daemonssets nicely.
  # Other jobs might need to be moved as well. We do this by
  # moving them to the sync phase. Additionally since the jobs
  # are using fixed names and not generated names for each run
  # ArgoCD attempts to edit them but they have immutable fields
  # so we must force the replacement instead of attempting to diff them.
  # Lastly the hook-delete-policy controls the finalizer which
  # prevents the deletion of the job. In this case we're saying
  # the old job needs to be removed before applying the new one
  # which gets around the immutable case above.
  job:
    nova_db_sync:
      argocd.argoproj.io/hook: Sync
      argocd.argoproj.io/hook-delete-policy: BeforeHookCreation
      argocd.argoproj.io/sync-options: Force=true
      argocd.argoproj.io/sync-wave: "0"
    nova_ks_service:
      argocd.argoproj.io/hook: Sync
      argocd.argoproj.io/hook-delete-policy: BeforeHookCreation
      argocd.argoproj.io/sync-options: Force=true
      argocd.argoproj.io/sync-wave: "0"
    nova_ks_user:
      argocd.argoproj.io/hook: Sync
      argocd.argoproj.io/hook-delete-policy: BeforeHookCreation
      argocd.argoproj.io/sync-options: Force=true
      argocd.argoproj.io/sync-wave: "0"
    nova_ks_endpoints:
      argocd.argoproj.io/hook: Sync
      argocd.argoproj.io/hook-delete-policy: BeforeHookCreation
      argocd.argoproj.io/sync-options: Force=true
      argocd.argoproj.io/sync-wave: "0"
    nova_cell_setup:
      argocd.argoproj.io/hook: PostSync
      argocd.argoproj.io/hook-delete-policy: BeforeHookCreation
      argocd.argoproj.io/sync-options: Force=true
    nova_bootstrap:
      argocd.argoproj.io/hook: Sync
      argocd.argoproj.io/hook-delete-policy: BeforeHookCreation
      argocd.argoproj.io/sync-options: Force=true
