---
bootstrap:
  image:
    enabled: false
    openstack:
      enabled: false
  network:
    enabled: false
    openstack:
      enabled: false
  object_store:
    enabled: false
    openstack:
      enabled: false

conductor:
  pxe:
    # at this time we are running our own dnsmasq container and statefulset
    enabled: false
  initContainers:
    # this can go away once we disable the ilo-ipxe and ipxe boot interfaces
    # it is only necessary because the above pxe is disabled, its init
    # creates this path
    - name: create-tftpboot
      image: ghcr.io/rackerlabs/understack/openstack-client:2025.2
      imagePullPolicy: IfNotPresent
      command: [bash]
      args:
        - "-c"
        - "mkdir -p /var/lib/openstack-helm/tftpboot /var/lib/openstack-helm/tmp"
      volumeMounts:
        - name: pod-data
          mountPath: /var/lib/openstack-helm

labels:
  conductor:
    node_selector_key: ironic_role
    node_selector_value: conductor

conf:
  # Update policies for better integration with OpenStack services
  # https://docs.openstack.org/ironic/latest/configuration/sample-policy.html
  policy:
    "baremetal:node:get:last_error": "role:service or role:admin or (project_id:%(node.owner)s or project_id:%(node.lessee)s)"
  ironic:
    DEFAULT:
      # We only want to default to direct, otherwise defaults interfere with hardware
      # types selecting their own defaults. So we purposefully leave the defaults unset
      # but enable everything that our Redfish focused (with iDRAC and iLO support)
      # systems need
      default_deploy_interface: direct
      enabled_bios_interfaces: no-bios,redfish,idrac-redfish,ilo
      enabled_boot_interfaces: http-ipxe,ipxe,redfish-virtual-media,redfish-https,idrac-redfish-virtual-media,ilo-virtual-media,ilo-uefi-https,ilo-ipxe
      enabled_deploy_interfaces: direct,ramdisk
      enabled_firmware_interfaces: redfish,no-firmware
      enabled_hardware_types: redfish,idrac,ilo5,ilo
      enabled_inspect_interfaces: redfish,agent,idrac-redfish,ilo
      enabled_management_interfaces: ipmitool,redfish,idrac-redfish,ilo,ilo5
      enabled_network_interfaces: noop,neutron
      enabled_power_interfaces: redfish,ipmitool,idrac-redfish,ilo
      enabled_raid_interfaces: redfish,idrac-redfish,ilo5,agent
      enabled_vendor_interfaces: redfish,ipmitool,idrac-redfish,ilo
      # the service account belongs to the service project but our nodes
      # will live in the infra domain in the baremetal project so the
      # service account needs to have permissions outside of just the
      # service project
      # see: https://review.opendev.org/c/openstack/ironic/+/907148
      rbac_service_role_elevated_access: true
    deploy:
      erase_devices_priority: 0
      erase_devices_metadata_priority: 0
      iso_master_path: /var/lib/openstack-helm/master_iso_images
    conductor:
      automated_clean: true
      clean_step_priority_override: deploy.erase_devices_express:95
      # (nicholas.kuechler) tuning for idrac hardware type
      # https://docs.openstack.org/ironic/latest/admin/drivers/idrac.html#nodes-go-into-maintenance-mode
      sync_power_state_interval: 70
    agent:
      # (nicholas.kuechler) tuning for idrac hardware type
      # https://docs.openstack.org/ironic/latest/admin/drivers/idrac.html#timeout-when-powering-off
      post_deploy_get_power_state_retry_interval: 18
    dhcp:
      dhcp_provider: dnsmasq
    oslo_messaging_rabbit:
      rabbit_ha_queues: true
    pxe:
      loader_file_paths: "snponly.efi:/usr/lib/ipxe/snponly.efi"
    inspector:
      extra_kernel_params: ipa-collect-lldp=1
      # Agent inspection hooks run after inspecting in-band using the IPA image:
      hooks: "ramdisk-error,validate-interfaces,architecture,pci-devices,parse-lldp,update-baremetal-port"
    redfish:
      # Redfish inspection hooks run after inspecting out-of-band using the BMC:
      inspection_hooks: "validate-interfaces,ports,port-bios-name,architecture,pci-devices,resource-class"
      add_ports: "all"
    # enable sensors and metrics for redfish metrics - https://docs.openstack.org/ironic/latest/admin/drivers/redfish/metrics.html
    sensor_data:
      send_sensor_data: true
      enable_for_undeployed_nodes: true
    metrics:
      backend: collector
    ilo:
      # we do not run swift by default so cannot use it by default
      use_web_server_for_images: true
    nova:
      auth_type: password
    vnc:
      enable: true
      container_provider: kubernetes
      console_image: ghcr.io/rackerlabs/understack/ironic-vnc-container:latest
      #  kubernetes_container_template: $pybasedir/console/container/ironic-console-pod.yaml.template

endpoints:
  oslo_messaging:
    namespace: null
    statefulset:
      replicas: 3
      name: rabbitmq-server
    hosts:
      default: rabbitmq-nodes
  baremetal:
    port:
      api:
        public: 443
    scheme:
      public: https
    host_fqdn_override:
      public:
        tls:
          secretName: ironic-tls-public
          issuerRef:
            name: understack-cluster-issuer
            kind: ClusterIssuer

secrets:
  tls:
    baremetal:
      api:
        # needs to be kept in sync with secretName in the host_fqdn_override
        # because helm-toolkit checks one field but then uses the other
        public: ironic-tls-public

network:
  api:
    external_policy_local: false
    node_port:
      enabled: false
  pxe:
    # hack to make things not fail on the default case
    device: lo

  # configure OpenStack Helm to use Undercloud's ingress
  # instead of expecting the ingress controller provided
  # by OpenStack Helm
  use_external_ingress_controller: true

dependencies:
  dynamic:
    common:
      local_image_registry:
        jobs: null
  static:
    api:
      jobs:
        - ironic-db-sync
        - ironic-ks-user
        - ironic-ks-endpoints
      services:
        - endpoint: internal
          service: oslo_db
        - endpoint: internal
          service: oslo_messaging
    conductor:
      jobs:
        - ironic-db-sync
        - ironic-ks-user
        - ironic-ks-endpoints
      services:
        - endpoint: internal
          service: oslo_db
        - endpoint: internal
          service: oslo_messaging
    # (nicholas.kuechler) The upstream helm values have ironic-db-init
    # as a dependency for db_sync, but we have job_db_init false so
    # ironic-db-init never gets created, and this dependency won't pass.
    # unset it, so that ironic-db-init is no longer listed in it.
    # https://opendev.org/openstack/openstack-helm/src/branch/master/ironic/values.yaml#L310-L312
    db_sync:
      jobs:

manifests:
  job_db_init: false
  job_db_drop: false
  job_manage_cleaning_network: false
  job_rabbit_init: false
  secret_db: false
  secret_rabbitmq: false
  secret_registry: false
  service_ingress_api: false
  service_ingress: false
  ingress_api: false
  # We set the `secret_keystone` and `secret_ks_etc` to false in order to disable
  # Kubernetes section generation in OpenStack Helm, because we want those
  # to be generated indirectly via ESO as configured in keystoneServiceUsers.enabled
  # that is later consumed via components/openstack helm chart
  secret_keystone: false
  secret_ks_etc: false
  # less than ideal but currently necessary due to the way the chart
  # hardcodes the config-file flag which then causes oslo.config to prefer
  # that data over the directory which we want to override
  configmap_bin: false

pod:
  mounts:
    ironic_conductor:
      ironic_conductor:
        volumeMounts:
          - name: dnsmasq-ironic
            mountPath: /etc/dnsmasq.d/
          - name: dnsmasq-dhcp
            mountPath: /var/lib/dnsmasq/
          - name: device-types
            mountPath: /var/lib/understack/device-types
          - name: ironic-etc-snippets
            mountPath: /etc/ironic/ironic.conf.d
            readOnly: true
        volumes:
          - name: dnsmasq-ironic
            persistentVolumeClaim:
              claimName: dnsmasq-ironic
          - name: dnsmasq-dhcp
            persistentVolumeClaim:
              claimName: dnsmasq-dhcp
          - name: device-types
            configMap:
              name: device-types
          - name: ironic-etc-snippets
            projected:
              sources:
                - secret:
                    name: ironic-ks-etc
          - name: pod-usr-share-novnc
            emptyDir: {}
    ironic_api:
      ironic_api:
        volumeMounts:
          - name: ironic-etc-snippets
            mountPath: /etc/ironic/ironic.conf.d
            readOnly: true
        volumes:
          - name: ironic-etc-snippets
            projected:
              sources:
                - secret:
                    name: ironic-ks-etc
  lifecycle:
    disruption_budget:
      api:
        # this should be set to no more than (pod.replicas.api - 1)
        # usually set on per-deployment basis.
        min_available: 0
  resources:
    enabled: true
    api:
      requests:
        memory: "1024Mi"
        cpu: "100m"
      limits:
        memory: "2048Mi"
        cpu: "500m"
    conductor:
      requests:
        memory: "1024Mi"
        cpu: "100m"
      limits:
        memory: "2048Mi"
        cpu: "1000m"

annotations:
  # we need to modify the annotations on OpenStack Helm
  # jobs because they use helm.sh/hooks: post-install,post-upgrade
  # which means they will get applied in the post phase which
  # is after the API deployment. With standard helm this works
  # out because it just orders how things are applied but with
  # ArgoCD it will wait until the sync phase is successful.
  # Unfortunately the API deployments need several jobs to occur
  # before it will go successful like creating the keystone user,
  # service, endpoints and syncing the DB. These jobs also have
  # a helm.sh/hook-weight to order them which is good but by moving
  # them to the sync phase the weight is now wrong with resources
  # they depend on like secrets and configmaps so we need to
  # override them to 0 because there is no way in OpenStack Helm
  # to set annotations on deployments and daemonssets nicely.
  # Other jobs might need to be moved as well. We do this by
  # moving them to the sync phase. Additionally since the jobs
  # are using fixed names and not generated names for each run
  # ArgoCD attempts to edit them but they have immutable fields
  # so we must force the replacement instead of attempting to diff them.
  # Lastly the hook-delete-policy controls the finalizer which
  # prevents the deletion of the job. In this case we're saying
  # the old job needs to be removed before applying the new one
  # which gets around the immutable case above.
  job:
    ironic_db_sync:
      argocd.argoproj.io/hook: Sync
      argocd.argoproj.io/hook-delete-policy: BeforeHookCreation
      argocd.argoproj.io/sync-options: Force=true
      argocd.argoproj.io/sync-wave: "0"
    ironic_ks_service:
      argocd.argoproj.io/hook: Sync
      argocd.argoproj.io/hook-delete-policy: BeforeHookCreation
      argocd.argoproj.io/sync-options: Force=true
      argocd.argoproj.io/sync-wave: "0"
    ironic_ks_user:
      argocd.argoproj.io/hook: Sync
      argocd.argoproj.io/hook-delete-policy: BeforeHookCreation
      argocd.argoproj.io/sync-options: Force=true
      argocd.argoproj.io/sync-wave: "0"
    ironic_ks_endpoints:
      argocd.argoproj.io/hook: Sync
      argocd.argoproj.io/hook-delete-policy: BeforeHookCreation
      argocd.argoproj.io/sync-options: Force=true
      argocd.argoproj.io/sync-wave: "0"
